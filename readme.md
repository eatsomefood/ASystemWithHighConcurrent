1:使用redis的Hash,Set,ZSet结构分别存储点赞数，点赞状态和用户点赞列表，
经过异步处理和定时任务补偿，
成功将时间从50ms+减低到20ms一下，
并且可以同时容纳十万+QPS

像 B 站（哔哩哔哩）这类高并发视频平台的点赞功能，需要兼顾实时性（用户操作后立即反馈）、高并发（百万级日活用户高频互动）、数据可靠性（不丢数据） 三大核心目标，其设计通常围绕 “缓存优先、异步落地、分层存储” 展开，具体实现可拆解为以下几个关键环节：
一、前端：实时反馈与防抖动
用户点击 “点赞” 按钮时，前端不会等待后端完整处理完成，而是先做本地状态更新（比如立即显示 “已点赞” 和点赞数 + 1），再异步发送请求到后端。同时会做简单的防抖动处理（比如 1 秒内重复点击只生效一次），减少无效请求。
二、后端：读写分离与异步化
1. 读操作：全量依赖缓存，保证实时性
   用户查看视频时，点赞数、“是否已点赞” 等信息直接从 Redis 读取，而非数据库，原因是：
   点赞属于高频读场景（用户刷视频时会频繁查看点赞数），Redis 的内存读写性能（单实例 10 万 + QPS）远高于 MySQL（单表几千 QPS）。
   存储结构设计（以视频 ID vid 和用户 ID uid 为核心）：
   点赞数计数：用 Hash 存储，例如 key=like:video:{vid}, field=count, value=点赞数（支持原子自增 HINCRBY）。
   用户点赞状态：用 Set 存储，例如 key=like:video:{vid}:users，值为点赞过该视频的 uid（判断用户是否点赞过用 SISMEMBER，O (1) 复杂度）。
   用户点赞列表：用 SortedSet 存储，例如 key=user:like:{uid}，值为 vid，分数为点赞时间（方便用户查看 “我的点赞” 并按时间排序）。
2. 写操作：先更缓存，再异步落库
   用户点击点赞 / 取消点赞时，后端处理流程为：
   Step 1：校验与缓存更新先通过 Redis 判断用户是否已点赞（SISMEMBER），避免重复操作。若为新点赞：
   原子性更新点赞数（HINCRBY like:video:{vid} count 1）；
   将用户 ID 加入视频的点赞用户集（SADD like:video:{vid}:users {uid}）；
   将视频 ID 加入用户的点赞列表（ZADD user:like:{uid} {timestamp} {vid}）。
   （取消点赞则反之：HINCRBY -1、SREM、ZREM）
   Step 2：异步同步到数据库缓存更新后，立即返回成功给前端（保证响应速度，通常 < 10ms），然后将操作详情（uid、vid、操作类型like/cancel、时间戳）写入消息队列（如 Kafka/RabbitMQ），由消费者服务异步同步到 MySQL。
   这样做的核心目的是：避免同步写入数据库的耗时（5-20ms）阻塞用户操作，通过消息队列削峰填谷（即使瞬间几十万点赞请求，也能匀速处理）。
   三、数据库：分层存储与分表优化
   MySQL 主要用于持久化存储点赞记录（防止 Redis 宕机丢失数据）和支持复杂查询（如 “查询某视频近 7 天新增点赞数”），设计上需解决 “海量数据存储” 问题：
1. 表结构设计
   核心表 like_records 字段：
   sql
   CREATE TABLE `like_records_{shard}` (
   `id` bigint(20) NOT NULL AUTO_INCREMENT,
   `user_id` bigint(20) NOT NULL COMMENT '用户ID',
   `video_id` bigint(20) NOT NULL COMMENT '视频ID',
   `status` tinyint(1) NOT NULL COMMENT '状态：1=点赞，0=取消',
   `create_time` datetime NOT NULL COMMENT '操作时间',
   PRIMARY KEY (`id`),
   UNIQUE KEY `uk_uid_vid` (`user_id`,`video_id`) COMMENT '防重复点赞'
   ) ENGINE=InnoDB DEFAULT CHARSET=utf8mb4 COMMENT '点赞记录（分表）';
2. 分表策略
   由于 B 站视频量和用户量极大（假设千万级视频、亿级用户），单表无法承载，需按 video_id 或 user_id 哈希分表（比如分 128 张表），分散写入压力。
3. 异步写入逻辑
   消息队列的消费者服务读取操作消息后，执行数据库更新：
   点赞：INSERT IGNORE 插入记录（利用 uk_uid_vid 唯一索引防重复），或更新 status=1；
   取消点赞：UPDATE 设 status=0（不物理删除，保留历史记录）。
   四、一致性保证：最终一致 + 校准机制
   由于 “先更缓存，再异步落库” 存在短暂的 Redis 与 MySQL 不一致（比如缓存更新后消息队列消费失败），需通过以下机制保证最终一致：
   消息队列重试机制：消费者处理失败时，消息会重新入队（设置最大重试次数，比如 5 次），避免因临时故障丢失数据。
   定期全量校准：每天凌晨（低峰期）启动定时任务，对比 Redis 中的点赞数与 MySQL 中 status=1 的记录数，差异部分进行补正（比如 Redis 少了则加，多了则减）。
   缓存失效兜底：Redis 中的点赞数据设置较长过期时间（如 7 天），但用户访问时若缓存失效，会先从数据库加载最新数据到 Redis（冷启动兜底）。
   五、防刷与限流
   作为 UGC 平台，点赞需防止恶意刷赞（如机器人批量点赞），通常结合以下措施：
   用户行为限流：单用户对同一视频的点赞操作 1 分钟内最多 1 次（Redis 的 SET {uid}:{vid}:like_limit EX 60 NX 实现）。
   设备 / IP 限流：对异常 IP（短时间内大量请求）进行临时封禁。
   风控模型：通过用户等级、历史行为等判断是否为正常用户，高风险用户的点赞可能延迟生效或需二次验证。
   总结：核心设计思路
   B 站这类平台的点赞功能，本质是 “以 Redis 为中心的实时读写层 + 以 MySQL 为中心的持久化存储层 + 消息队列连接的异步同步层” 的组合，通过以下方式平衡性能与可靠性：
   读走缓存，保证高并发实时响应；
   写先更缓存再异步落库，避免阻塞用户；
   数据库分表 + 唯一索引，支撑海量数据存储；
   消息重试 + 定期校准，保证最终一致性。
   这种设计可支撑单视频每秒数千次点赞（热点视频通过 Redis 集群扩展），整体平台日均亿级点赞操作无压力。

// 延迟缓存策略 
总结：Redis 存储静态数据 “浪费与否”，关键在 “设计策略”
结论：只要策略合理，Redis 存储静态数据是 “提升性能的关键手段”，而非 “浪费”；所谓 “浪费” 均是因 “盲目缓存、未做取舍” 导致，可通过优化规避。
核心设计原则：
只缓存 “高频查询” 的数据，不缓存冷数据；
用 “小内存成本” 换 “大性能收益”（如压缩、LRU 淘汰）；
减少不必要的缓存操作（如避免无效更新、合理设置过期时间）。